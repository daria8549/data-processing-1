{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8e9fd436f8738876cded8d1bc870c35",
     "grade": false,
     "grade_id": "cell-2db14fa210da2ef0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea0ae3366f3db793f78a4c7fccec47cd",
     "grade": false,
     "grade_id": "cell-547da1b1777ed8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Assignment 3 (Individual)\n",
    "\n",
    "\n",
    "Continuing from Assignment 2, we will now further practice processing datasets.\n",
    "From what you have seen in the lecture, in this assignment you should practice the following steps:\n",
    "\n",
    "* basic dataset decription & assessing \"tidyness\"\n",
    "* filtering\n",
    "* sorting\n",
    "* aggregation\n",
    "\n",
    "using both \"pure\" Python and also using the pandas package. \n",
    "\n",
    "-----\n",
    "\n",
    "## Step 1a (4 points)\n",
    "\n",
    "Find a CSV dataset online (similar requirements apply as for Assignment 2), but additionally make sure that the dataset has \n",
    " * at least one non-numeric column (with a categorical variable), and\n",
    " * at least one column with numerical values.\n",
    "\n",
    "Note, as opposed to working in a group for Assignment 2, you are not allowed to use the same dataset, i.e., we expect everybody to find their own CSV file.\n",
    "\n",
    "Save the file into your data folder and name it __data_notebook-1_DataFile.csv__.\n",
    "\n",
    "*<b>ATTENTION</b>:* to avoid any unexpected errors when running your submission in our grading environment, we strongly recommend using an RFC-compliant CSV file encoded in `utf-8`: i.e., make sure that the CSV file uses ',' as a delimiter (if the original file doesn't, you may need to convert it first: i.e., our hidden tests assume that the code will work for any RFC-compliant CSV file with the delimiter ','.*). Lastly, please avoid CSV files with completely empty columns, i.e., each column should contain at least one value.\n",
    "\n",
    "Using the CSV package, write a function `analyzeCSV` to return\n",
    "* the number of rows in the dataset (excl. the header row).\n",
    "* How many different __values__ and what __datatype__ appear per column in the CSV file.\n",
    "\n",
    "That is, the function should return a dictionary of the following structure:\n",
    "```\n",
    " {\n",
    "  \"rows\": ..., # the number of rows (integer)\n",
    "  \"columns\": ... # a list of pairs [ (v_1,dt_1) , (v_2,dt_2), ... (v_1,dt_1)]\n",
    " }\n",
    "```\n",
    "`(v_i,dt_i)` denotes the number of different values (`v_i`) and the datatype (`dt_i`) detected in column `i`.\n",
    "If you detect several different datatypes per column your function should return the value `\"object\"` for `dt_i`. Missing value notations like `NaN` should be considered as strings. Also, you do not have to specifically test for type `datetime`.\n",
    "<br><br>\n",
    "\n",
    "Additionally, we have uploaded a CSV testfile in `unit3/data/testfile.csv` that you can use to check your solution against our target in the visible test cases. Make sure to put it in your `/data/` folder, to make the respective tests work; it also recommended to take a look at the visible test case results for all tasks, to see how you should format your output.\n",
    "\n",
    "__Hint__: For (heuristic) datatype detection per column, please use/adapt the function `convert()` from the notebook `unit3/00_value-transformation.ipynb` from the lecture, i.e., possible return values besides the string `\"object\"` should be the return value of the function `type()` applied to the heuristically converted values in the column using `convert()`.\n",
    "\n",
    "__Caution__: Do not import any other packages than those mentioned in lecture examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d62e8e000dbcb6db758a0b22a54ccb4",
     "grade": false,
     "grade_id": "cell-888abc9fb6a6b742",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "filePath = \"./data/testfile.csv\"\n",
    "testfile = \"./data/testfile.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8887ebd91b2c096e9badab37fd8e2cf",
     "grade": false,
     "grade_id": "cell-2b88d1e6c1925d1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filePath = \"./data/data_notebook-1_DataFile.csv\"\n",
    "\n",
    "def analyzeCSV(filePath):\n",
    "    row_count = 0\n",
    "    values = []\n",
    "    types = []\n",
    "    \n",
    "    with open(filePath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)  #excl. header\n",
    "        column_count = len(header)  \n",
    "        \n",
    "        for v in range(column_count): \n",
    "            # 'set' removes duplicating values\n",
    "            values.append(set())  # creating set for storing unique values from the column\n",
    "            types.append(set())   # creating set for storing data types \n",
    "        \n",
    "        for row in reader:\n",
    "            row_count += 1\n",
    "            for i in range(column_count):\n",
    "                value = row[i]\n",
    "                values[i].add(value)  # going through every row ain every column and storing values and types\n",
    "                \n",
    "                column_type = convert(value)\n",
    "                types[i].add(column_type)\n",
    "\n",
    "    columns = []\n",
    "    for i in range(column_count):\n",
    "        if len(types[i]) == 1:   # if there is only 1 data type, we add to the list\n",
    "            columns.append((len(values[i]), types[i].pop()))\n",
    "        else:\n",
    "            columns.append((len(values[i]), \"object\")) #if there is multiple types in a single column, it will be classified as \"object\"\n",
    "            \n",
    "\n",
    "    result = {\"rows\":row_count, \"columns\":columns}\n",
    "    return result\n",
    "            \n",
    "\n",
    "def convert(value): #convert function from the 1st unit\n",
    "    try:\n",
    "        int(value)\n",
    "        return \"int\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        float(value) \n",
    "        return \"float\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if value == \"\" or value.lower() == \"nan\":\n",
    "        return \"str\"\n",
    "    \n",
    "    return \"str\"\n",
    "\n",
    "#analyzeCSV(testfile)\n",
    "#analyzeCSV(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86276cf50755139e53f8a576fded20dd",
     "grade": false,
     "grade_id": "cell-ab8cd6999e00795a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert_equal(type(analyzeCSV(filePath)), dict)\n",
    "assert_equal(len(analyzeCSV(filePath)), 2)\n",
    "assert_equal(analyzeCSV(testfile), {'rows': 12, 'columns': [(3, 'str'), (3, 'int'), (2, 'str'), (6, 'int')]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "678785b1463c6887e7d54045455acc1e",
     "grade": false,
     "grade_id": "cell-0601eda65c85a474",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now (possibly with some more manual inspection) answer the following questions:\n",
    "* Which types of variables appear in this dataset? For each variable, indicate whether\n",
    "  *  it is numerical or categorical; for numeric values, specify whether the scale is nominal, ordinal, interval, or ratio.\n",
    "  *  it is an identifier, a dimension, or a measurement.  \n",
    "* How would you describe an \"observation\" in this dataset?\n",
    "* Is the dataset tidy?\n",
    "* If not, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95b0f6134ade6ae1b5f2bc193b820d77",
     "grade": true,
     "grade_id": "cell-f2d935f6c5393e81",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "* Categorical:\n",
    "     * Nominal: name, country, category \n",
    "     \n",
    "* Numerical:\n",
    "     * Ordinal: rownames, rank\n",
    "     * Ratio: sales, profits, assets, marketvalue\n",
    "     \n",
    "* Identifiers:rownames, rank, name\n",
    "* Dimensions: country, category\n",
    "* Measurements: sales, profits, assets, marketvalue\n",
    "\n",
    "An \"observation\" in this dataset is a company, characterized by ots name and net worth measures, as well as categoty and country. Each observation gives an insight on company's ranking and financial status.\n",
    "\n",
    "Yes, the dataset is tidy:\n",
    "Each row is single observation (corresponds to data of exactly 1 company).\n",
    "Each value is sorted and in the right place.\n",
    "Every column represents a single variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7816728ad695a6e37bbab858b740b530",
     "grade": false,
     "grade_id": "cell-92fae8cb742fa6bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 1b (2 points)\n",
    "\n",
    "Now, perform the same steps you solved in 1a using pandas, i.e., read in the CSV file and write the function `pandalyzeCSV` that computes the same structure as defined in 1a.\n",
    "\n",
    "__Hint:__ pandas dataframes provide the useful function `nunique()` to compute the number of unique values in a column (`v_i`) as well as the attribute  `dtypes` to get the column datatypes (`dt_i`), which you should use. This means, that the result returned by this solution using pandas might be slighlty different than the solution of step 1a.\n",
    "\n",
    "Using the test CSV provided by us, your solution for `pandalyzeCSV` should result in this dictionary:\n",
    "\n",
    "`{'rows': 12, 'columns': [(3, dtype('O')), (3, dtype('int64')), (2, dtype('O')), (6, dtype('int64'))]})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a926db539d1cc4eaa65315445d7151a",
     "grade": false,
     "grade_id": "cell-a8850bfaa5ec668e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pandalyzeCSV(filePath):\n",
    "    df = pd.read_csv(filePath)\n",
    "    row_count = df.shape[0]\n",
    "    columns = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        values_count = df[col].nunique()\n",
    "        column_type = df[col].dtype\n",
    "        \n",
    "        columns.append((values_count, column_type))\n",
    "    \n",
    "    result = {\n",
    "        \"rows\": row_count,\n",
    "        \"columns\": columns\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64c4d7ff0b546bf9409b7180e7f90baa",
     "grade": false,
     "grade_id": "cell-c27d203b1fadc3ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(pandalyzeCSV(filePath)), dict)\n",
    "assert_equal(len(pandalyzeCSV(filePath)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c07a9206adae8e3a8bac0d6c3baec3c",
     "grade": false,
     "grade_id": "cell-763a3cdfc1a5447d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 2a Filtering (2 points)\n",
    "\n",
    "Using pure Python, i.e. NOT pandas, write the function csvFilter, which should:\n",
    "\n",
    "* convert the CSV to a list of lists, where each row becomes an inner list;\n",
    "* filter the rows of the dataset by the <b>numeric</b> column with the most unique values (if there are several columns with the same number of unique values, take the left-most of those) by showing all rows where the value of that column is less than or equal to the median of the column;\n",
    "* return the list of lists containing only the filtered rows and no header!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cd9fa6da9b56b75c87cba26e8d46b40",
     "grade": false,
     "grade_id": "cell-3e8023888dd8a9c3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#filePath = \"./data/data_notebook-1_DataFile.csv\"\n",
    "\n",
    "def csvFilter(filePath):\n",
    "    with open(filePath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        \n",
    "        header = data[0]\n",
    "        rows = data[1:]\n",
    "        \n",
    "        num_column = []\n",
    "        for i in range(len(header)):\n",
    "            values_column = []\n",
    "            is_numeric = True\n",
    "            \n",
    "            for row in rows:  # identifing numeric colums\n",
    "                try:\n",
    "                    values_column.append(float(row[i])) # if it can be converted to the float, column found\n",
    "                except ValueError:\n",
    "                    is_numeric = False  \n",
    "                    break # stops checking the column if non-numeric value was founded\n",
    "            \n",
    "            if is_numeric:  #adding fully numeric columns to set\n",
    "                unique_values = set(values_column)\n",
    "                num_column.append((i, len(unique_values), values_column))\n",
    "        \n",
    "        # Find the column with the most unique values\n",
    "        max_index = 0\n",
    "        max_count = num_column[0][1]\n",
    "        for i in range (len(num_column)): #finding out wich column has the highest unique value count\n",
    "            if num_column[i][1] > max_count:\n",
    "                max_index = i\n",
    "                max_count = num_column[i][1]\n",
    "        \n",
    "        \n",
    "        selected_column = num_column[max_index]  # Get the tuple for the column with the most unique values\n",
    "        max_index = selected_column[0]  # The actual column index\n",
    "        max_value = selected_column[2]  #the list of numeric values in that column, in a tuple\n",
    "        \n",
    "        sorted_list = sorted(max_value)\n",
    "        \n",
    "        n = len(sorted_list)\n",
    "        if n%2 != 0:\n",
    "            median = sorted_list[n//2]\n",
    "        else:\n",
    "            median = (sorted_list[n//2-1] + sorted_list[n//2])/2\n",
    "        \n",
    "        \n",
    "        filtered = []\n",
    "        for row in rows:\n",
    "            try:\n",
    "                if float(row[max_index]) <= median:\n",
    "                    if row not in filtered:  # Check if the row is already added\n",
    "                        filtered.append(row)\n",
    "            except ValueError:\n",
    "                continue \n",
    "        return filtered\n",
    "            \n",
    "#csvFilter(testfile)\n",
    "#csvFilter(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c77a5fafdc44651f1008a1093fde50f",
     "grade": false,
     "grade_id": "cell-bd52f1acb2358696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(csvFilter(testfile)), list)\n",
    "assert_equal(type(csvFilter(filePath)), list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5e2927d1cc5eab17075cf162b6ae8a7",
     "grade": false,
     "grade_id": "cell-ccb23da90625904d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 2b Filtering - pandas (2 points)\n",
    "\n",
    "Now, again using pandas, write the function `pandasFilter` to:\n",
    "* convert the dataset to a pandas dataframe\n",
    "* filter the rows of the dataset by the same condition as for 2a:\n",
    "* return the pandas dataframe only containing the filtered rows! The header and index can remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "237a0d5715d0811eba48932a669fdc81",
     "grade": false,
     "grade_id": "cell-4822ef9f00f9e26c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pandasFilter(filePath):\n",
    "    df = pd.read_csv(filePath)\n",
    "    numeric_df = df.select_dtypes(include=['number'])  # Selecting only numeric columns\n",
    "    unique_counts = numeric_df.nunique()  # Counting unique values per numeric column\n",
    "    most_unique_column = unique_counts.idxmax()  # Finding the column with the most unique values\n",
    "    median_value = numeric_df[most_unique_column].median()\n",
    "    filtered_df = df[df[most_unique_column] <= median_value]  #Filtering rows where the value in the most unique column is <= median\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83b4a16035aedc416d71a41d57095402",
     "grade": false,
     "grade_id": "cell-4e7f425477a41440",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(pandasFilter(testfile)), pd.DataFrame)\n",
    "assert_equal(type(pandasFilter(filePath)), pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed7e13d98c55cd6952b7352216fc7938",
     "grade": false,
     "grade_id": "cell-8c1be8eef825aa09",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 3a Sorting (2 points)\n",
    "\n",
    "Using pure Python, i.e. NOT using pandas:\n",
    "* define a function sortCSV(filePath) which sorts the rows of your csv file by the left-most numerical column in descending order of values.\n",
    "* The function takes *filePath* as input (which you have already defined above).\n",
    "* Your function should return the sorted contents of the CSV file as a list of lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f94e91f53ae35128b19e4c672daa6977",
     "grade": false,
     "grade_id": "cell-987ea095365b9681",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sortCSV(filePath):\n",
    "    with open(filePath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "\n",
    "    header = data[0]\n",
    "    rows = data[1:]\n",
    "\n",
    "    numeric_index = 0   # Identifing the first numeric column\n",
    "    for i in range(len(header)):\n",
    "        try:\n",
    "            float(rows[0][i])\n",
    "            numeric_index = i    #If there is no errors, then we take the first column, that was successfully converted \n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    for i in range(len(rows)):    # Selection sort in descending order\n",
    "        max_index = i\n",
    "        for j in range(i, len(rows)):\n",
    "            try:\n",
    "                if float(rows[j][numeric_index]) > float(rows[max_index][numeric_index]):\n",
    "                    max_index = j\n",
    "            except ValueError:     # Skiping rows that cannot be converted to float\n",
    "                continue\n",
    "        rows[i], rows[max_index] = rows[max_index], rows[i]\n",
    "\n",
    "    result = [header] + rows\n",
    "\n",
    "    return result\n",
    "\n",
    "#sortCSV(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02b5a751ad0c518a29df6aa0366e8515",
     "grade": false,
     "grade_id": "cell-a334c571e3d33bf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(sortCSV(testfile)), list)\n",
    "assert_equal(type(sortCSV(filePath)), list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43e04844dae1ff76c83eff49c28a982a",
     "grade": false,
     "grade_id": "cell-1eff3872e1df78c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 3b Sorting - pandas (2 points)\n",
    "\n",
    "Now, again using pandas, \n",
    "* define a similar function PandasSortCSV(filePath) which sorts the rows of the dataset by the same columns as in 3a, again in descending order of values. \n",
    "* This time, your function should return a pandas dataframe.\n",
    "  \n",
    "*Hint*: There is no need to reset the index, just leave it as it is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "433a3e7a697b9d130d4bd0046795ef11",
     "grade": false,
     "grade_id": "cell-e63244f3196ef27f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def PandasSortCSV(filePath):\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    numeric_column = 0\n",
    "    for column in df.columns:\n",
    "        if pd.to_numeric(df[column], errors='coerce').notna().all():  # If the entire column is numeric\n",
    "            numeric_column = column\n",
    "            break\n",
    "    \n",
    "    sorted_df = df.sort_values(by=numeric_column, ascending=False)\n",
    "    return sorted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63d9f55b9ae1bca8ba535fbdce73c53c",
     "grade": false,
     "grade_id": "cell-76f4d0fa5f0d1c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(isinstance(PandasSortCSV(testfile), pd.DataFrame), True)\n",
    "assert_equal(isinstance(PandasSortCSV(filePath), pd.DataFrame), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a393d3978fbc859f4c646012704c184f",
     "grade": false,
     "grade_id": "cell-b64c2a5902f2cd4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 4a Aggregation and Grouping (4 points)\n",
    "\n",
    "Using pure Python, i.e. NOT using pandas define a function `aggregateCSV(filePath)` which does the following:\n",
    "* takes the left-most numeric column _X_ and the left-most non-numeric column _Y_ and\n",
    "* calculates the __average (mean)__ of the values in column _X_ for each value in column _Y_.\n",
    "\n",
    "Your function should return a dictionary with the unique values of the non-numeric column as keys and the averages (per group) of the numeric column as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "decebefd5ef7528eeaab366c68640f59",
     "grade": false,
     "grade_id": "cell-af18f5282baa2eb7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def aggregateCSV(filePath):    \n",
    "    with open(filePath) as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        \n",
    "        header = data[0]\n",
    "        rows = data[1:]\n",
    "        \n",
    "        numeric_index = None\n",
    "        non_numeric_index = None\n",
    "        \n",
    "        for i in range (len(header)):\n",
    "            try:\n",
    "                float(rows[0][i])\n",
    "                numeric_index = i    #If there is no errors, then we take the index of the first column, that was successfully converted \n",
    "            except ValueError:\n",
    "                if non_numeric_index == None: # In contraty, the first column that wasnt successfuly converted is non_numeric\n",
    "                    non_numeric_index = i\n",
    "                continue\n",
    "        \n",
    "        values = {} # dictinary with key and all found values\n",
    "        for row in rows:\n",
    "            try:\n",
    "                numeric_value = float(row[numeric_index])  # here we insert the index and find the according value\n",
    "                non_numeric_value = str(row[non_numeric_index])\n",
    "                \n",
    "                if non_numeric_value not in values:   #if there is no key with Non_numeric_value\n",
    "                    values[non_numeric_value] = []    # it creates such key, with an empty list as a value\n",
    "                values[non_numeric_value].append(numeric_value) # and now we can add all numeric values that have the same non_numeric key\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        result = {}  # dictinary with key and mean of all found values\n",
    "        for key, values in values.items():\n",
    "            mean = sum(values)/len(values)\n",
    "            result[key] = mean\n",
    "                \n",
    "        return result\n",
    "\n",
    "#aggregateCSV(testfile) \n",
    "#aggregateCSV(filePath)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f0addb5e3ee6b693a929d2d12433566",
     "grade": false,
     "grade_id": "cell-342b283e07825927",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(aggregateCSV(testfile)), dict)\n",
    "assert_equal(type(aggregateCSV(filePath)), dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc2a0c21f0dd864d218a3c59ebc029cf",
     "grade": false,
     "grade_id": "cell-ddbe334d3b102c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 4b Aggregation and Grouping - pandas (2 points)\n",
    "\n",
    "Now, again using pandas, do the same as in 4a and call the function `PandasAggregateCSV` this time. It should return a pandas dataframe. The columns do not have to be renamed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1941355b2a28767d087f43d822d7848f",
     "grade": false,
     "grade_id": "cell-95103e5f3482a467",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def PandasAggregateCSV(filePath):\n",
    "    df = pd.read_csv(filePath)\n",
    "    \n",
    "    numeric_column = None\n",
    "    non_numeric_column = None\n",
    "    \n",
    "    for column in df.columns:\n",
    "        try:                            #converting to float to check if the column numeric\n",
    "            float(df[column].iloc[0])   # [0] it is accessing the first row of the specified column\n",
    "            if numeric_column is None:\n",
    "                numeric_column = column\n",
    "        except ValueError:              # If it fails, it is non-numeric\n",
    "            if non_numeric_column is None:\n",
    "                non_numeric_column = column\n",
    "                \n",
    "        if (numeric_column != None) and (non_numeric_column != None):\n",
    "            break                       # break when columns are found\n",
    "            \n",
    "    result = df.groupby(non_numeric_column)[numeric_column].mean().reset_index()   #Tutorium 3\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa639264dd70422956e559558202a36b",
     "grade": false,
     "grade_id": "cell-e97bad506966067e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(isinstance(PandasAggregateCSV(testfile), pd.DataFrame), True)\n",
    "assert_equal(isinstance(PandasAggregateCSV(filePath), pd.DataFrame), True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
