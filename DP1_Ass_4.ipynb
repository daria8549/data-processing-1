{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2837835817b53736d9363b302b300709",
     "grade": false,
     "grade_id": "cell-420b4c449379ffe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0. Import\n",
    "\n",
    "Implement a function `tidy` which imports the data set assigned and provided to you as a CSV file into a `pandas` dataframe. Access the data set and establish whether your data set is tidy. If not, clean the data set before continuing with Step 1. Mind all rules of tidying data sets in this step. Make sure you comply to the following statements:\n",
    "* If there is an index column (row numbers) in your tidied dataset, keep it.\n",
    "* The following columns, once identified, correspond to variables 1:1 (no need for transformations):\n",
    "  * `full_name`\n",
    "  * `automotive`\n",
    "  * `color`\n",
    "  * `job`\n",
    "  * `address`\n",
    "  * `coordinates`\n",
    "  * `km_per_litre`\n",
    "* The tidied dataset should have a total of 9 columns (not including the index), the first column should be `full_name`.\n",
    "* Mind the intended content of each attribute (e.g. full_name should contain the full name of a person, no need to change that)\n",
    "* If tidy or done, have the function `tidy` return the ready data set as a dataframe.\n",
    "\n",
    "Note that `tidy` must take a single parameter that holds the basename of the CSV file (i.e., the name without file extension). Do NOT change the name of the file, do not overwrite the original data file, and make sure you submit your final ZIP following the Code of Conduct (CoC) requirements. Especially, make sure you put your data file in a folder called `data/` when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e96fea9cc9f84652b592c3659339a48b",
     "grade": false,
     "grade_id": "cell-81e21dccd785e63d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tidy(x):\n",
    "    df = pd.read_csv(f\"./data/{x}.csv\", header=None)    \n",
    "    if df.shape[0] < df.shape[1]:  # If there are more columns than rows, it's structured horizontally\n",
    "        df = df.transpose()        # swaping the rows and columns\n",
    "        df.columns = df.iloc[0]    # seting the first row as the header\n",
    "        df = df[1:]                # removing the first row as it's now the header\n",
    "    \n",
    "    # Split 'date_time/full_company_name' into 'date_time' and 'company_name'\n",
    "    df[['date_time', 'company_name']] = df['date_time/full_company_name'].str.extract(r'([0-9\\- :\\.]+)([A-Za-z\\s,&]+)$')\n",
    "    \n",
    "    df = df.drop(columns=['date_time/full_company_name'])\n",
    "    \n",
    "    required_columns = ['full_name', 'automotive', 'color', 'job', 'address', 'coordinates', \n",
    "        'date_time', 'company_name', 'km_per_litre']\n",
    "    df = df[required_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "#tidy(mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(type(tidy(mn)), pd.core.frame.DataFrame)\n",
    "assert_equal(len((tidy(mn)).columns), 9)\n",
    "assert_equal(list((tidy(mn)).columns)[0], \"full_name\")\n",
    "assert_equal(list((tidy(mn)).columns)[len((tidy(mn)).columns)-1], \"km_per_litre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "777ebb2a7693c48f125c022fc3e281cc",
     "grade": false,
     "grade_id": "cell-72c2573051ab8535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-------\n",
    "## 1. Missing values\n",
    "\n",
    "### 1.1 Code part\n",
    "Implement a function called `missing_values` which takes as an input a dataframe and check if there are any missing values in the dataset. Record the row ids of the observations containing missing values as a list of numbers and make sure that the function returns the recorded list in the end. If there are no missing values, `missing_values` should return an empty list.\n",
    "\n",
    "**NOTE:** You shall find out how missing values are encoded in your datasest and which missing values occur in your dataset, you probably will ***need manual inspection***. For instance they could be encoded as: `\"nan\"`,`\"(+/-)inf\"` but also other values or empty fields or fields containing only white spaces are conceivable to encode missing values in your dataset. (We are aware that this test generic test might be overshooting in practice ;-))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "194585e256b6899da4d2a52558bcf1d1",
     "grade": false,
     "grade_id": "cell-6d4fb7f2e44e7cfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def missing_values(x):\n",
    "    df_str = x.astype(str)     # srt to handle potential white space issues\n",
    "    \n",
    "    condition = df_str.isin([\"nan\", \"NaN\", \"\", \" \",\"-\" \"inf\", \"-inf\", \"+inf\"])   # condition for missing values\n",
    "    \n",
    "    missing_values = x.index[condition.any(axis=1)].tolist()\n",
    "    #display(x.loc[missing_values])\n",
    "    return missing_values\n",
    "\n",
    "#missing_values(tidy(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(type(missing_values(tidy(mn))), list)\n",
    "assert_equal(all(isinstance(i, int) for i in missing_values(tidy(mn))), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52fac7bcbfdfcaea74bdcda572dac51b",
     "grade": false,
     "grade_id": "cell-82f316abfa9423e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Analytical part\n",
    "\n",
    "* Does the dataset contain missing values?\n",
    "* If no, explain how you proved that this is actually the case.\n",
    "* If yes, describe the discovered missing values. What could be an explanation for their missingness?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229a22e8b0e33f07eb7e3e7a6a652bf2",
     "grade": true,
     "grade_id": "cell-3ad38c6f2d1998f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. Yes there are missing values in the dataset.\n",
    "2. We have Nan, -inf, +inf, - , \" \".\n",
    "\n",
    "NaN: typical example of missing data in Python. It means that data was not provided/recorded in the first place.\n",
    "\n",
    "+inf/-inf: data was entered incorrectly.\n",
    "\n",
    "We have missing data in following columns: full_name, automotive, job, address, coordinates, data_time, company_name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "828e5a07966e9e14239edd87b539003c",
     "grade": false,
     "grade_id": "cell-87126fc406d941ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## 2. Handling missing values\n",
    "### 2.1. Code part\n",
    "Apply a (simple) function called *handling_missing_values* for handling missing values using an adequate single-imputation technique  of your choice per type of missing values. Make use of the techniques learned in Unit 4. The function should take as an input a dataframe and return the updated dataframe. Mind the following:\n",
    "- The objective is to apply single imputation on these synthetic data. Do not make up a background story (at this point)!\n",
    "- Do NOT simply drop the missing values. This is not an option.\n",
    "- The imputation technique must be adequate for a given variable type (quantitative, qualitative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0226044c2b66de952ca20bc64edae12b",
     "grade": false,
     "grade_id": "cell-5ba2dc8b5cbe1f8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def handling_missing_values(x):\n",
    "    x.replace(['+inf', '-inf', '-', ' ', ''], np.nan, inplace=True) #raplacing with NaN\n",
    "    \n",
    "    if 'date_time' in x.columns:\n",
    "        original_index = x.index\n",
    "        x['date_time'] = pd.to_datetime(x['date_time'])             # converting srting into datetime\n",
    "        x = x.sort_values(by='date_time')                           # sorting in hronological order for later use of LOCF\n",
    "        x['date_time'].fillna(method='ffill', inplace=True)         # LOCF(Last Observation Carried Forward) to fill missing date values\n",
    "        x = x.loc[original_index]\n",
    "    for column in x.columns:                                        # for qualitative data - mode(most frequent value)\n",
    "            if not x[column].mode().empty:\n",
    "                mode_value = x[column].mode()[0]                    # Get the first mode value\n",
    "            else:\n",
    "                mode_value = 'Unknown'\n",
    "            x[column].fillna(mode_value, inplace=True)\n",
    "    return x\n",
    "\n",
    "#handling_missing_values(tidy(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8e8a66c15721d5a8de6784be389229c",
     "grade": true,
     "grade_id": "cell-0edce052e98e2e95",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(missing_values(handling_missing_values(tidy(mn)))), 0)\n",
    "assert_equal(handling_missing_values(tidy(mn)).shape, tidy(mn).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "379b1e101131334e3262e96f723fe8e6",
     "grade": false,
     "grade_id": "cell-c697641b9d5a1c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2. Analytical part\n",
    "Discuss the implications. Answer the following:\n",
    "\n",
    "- How would you qualify the data-generating processes leading to different types of missing values, provided that the data was not synthetic?\n",
    "- What are the benefits and disadvantages of the chosen single-imputation technique?\n",
    "- How would you apply a multiple-imputation technique to one type of missing values, if applicable at all?\n",
    "- We asked you to test for/treat as missing values by checking certain field values, as well as empty fields or fields containing the numeric value 0... what are potential problems of this heuristics?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34f9ca681c722ebe151d6fac42d71b25",
     "grade": true,
     "grade_id": "cell-5c05456587f2ff17",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We have missing data in following columns: full_name, automotive, job, address, coordinates, data_time, company_name.\n",
    "So my guess that it's most likely MCAR: The \"process\" having produced missing data is independent from the (other) variables in a dataset.\n",
    "##### Processes that could lead to a missing data in our table:\n",
    "1. Equipment needed for measurement suffered a malfunction or service outage.\n",
    "2. Unplanned termination of data collection.\n",
    "Generally: The probability of a value being missing is unrelated to the value itself or any other variable in the data set. Therefore, the missing data is said being missing completely at random (MCAR).\n",
    "\n",
    "##### Single imputation (a.k.a. single substitution):\n",
    "I used mode imputation for qualitative variables: Substitute the most common value (mode) for the missing values of a given variable.\n",
    "Pros: Allows for complete-case analysis, does typically not add additional bias MCAR (wich I belive is the case here).\n",
    "Cons: adds bias when MAR and MNAR; replacement values are all identical, so the variances end up smaller than in real data.\n",
    "\n",
    "Most of missing values are categorical, but there are some numerical values in data_time. There I decided to use LOCF(Last Observation Carried Forward) to fill missing date values, as we did in tutorium. But there was a problem data wasn't structured as time series, so first I filtered data in hronological order, then filled the missing values and finally returned initial order. LOCF may also be seen as special case of hot-deck imputation.\n",
    "\n",
    "##### Random Hot-Deck Without Predictors:\n",
    "\n",
    "While in my opinion we dont have any valid predictors for the full_name or automotive  (in theory \"job\" column can be used, but not in this case, while it's seems to be a unique identifier), so Random Hot-Deck Without Predictors could be used for full_name column (for example). First I would have created donor table with all non-missing values from the column full_name and than would draw at random values to fill the missing one.\n",
    "\n",
    "##### If we handled values '0' as NaN:\n",
    "\n",
    "In this table specifically I could have affected only km_per_litre column. But generally:\n",
    " - it could have been a valid value\n",
    " - if it was the only heuristics, I would have overlooked other non-standard missing values such as '+/-inf', ' '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7219b50c7040d873e766fd1e497c1240",
     "grade": false,
     "grade_id": "cell-573d56d6699b84eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 3. Detecting duplicate entries\n",
    "Implement a function called `duplicates` that takes as an input a (tidy) dataframe `x`. Assume that `duplicates` receives a dataframe as returned from your Step 0 implementation of `tidy`. It then checks whether there are any duplicates in the dataset. Record the row ids of the observations being duplicates and have `duplicates` returns the list in the end. An empty list indicates the absence of duplicated observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d3199cb48685917455f085c8d366844",
     "grade": false,
     "grade_id": "cell-7954cffea933a812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def duplicates(x):\n",
    "    x = x.astype(str)\n",
    "    duplicates = x[x[['full_name', 'company_name']].duplicated()].index.tolist()\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(type(duplicates(tidy(mn))), list)\n",
    "assert_equal(all(isinstance(i, int) for i in duplicates(tidy(mn))), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "daeba952866630dc3bbeb0dcbc19b953",
     "grade": false,
     "grade_id": "cell-b04e3f6689a78a44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 4. Detecting outliers\n",
    "### 4.1. Code part\n",
    "Implement a function called `detecting_outliers` to detect outliers in one selected quantitative variable. Pick a suitable variable from the tidied dataset based on your characterisation and apply one suitable outlier-detection technique as covered in Unit 4. Justify your choice of this technique in the analytical part. Again, the function is assumed to receive a tidied data set from Step 0. The function returns the row ids of the rows containing outliers on the selected variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c10ec065604cff85d708868fa0909ced",
     "grade": false,
     "grade_id": "cell-f593e79eae8f4227",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def detecting_outliers(x, distance = 0.5):\n",
    "    var = 'km_per_litre'\n",
    "    x[var] = pd.to_numeric(x[var], errors='coerce')\n",
    "    q25 = x[var].quantile(0.25)\n",
    "    q75 = x[var].quantile(0.75)\n",
    "    #print(f\"Q1 (25th percentile): {q25}, Q3 (75th percentile): {q75}\")\n",
    "    IQ = q75 - q25\n",
    "    minval = q25 - distance*IQ\n",
    "    maxval = q75 + distance*IQ\n",
    "    #print(f\"IQR: {IQ}, Min value: {minval}, Max value: {maxval}\")\n",
    "    outliers = x[(x[var] < minval) | (x[var] > maxval)]\n",
    "    index = outliers.index.tolist()\n",
    "    #print(len(index))\n",
    "    #display(x.loc[index])\n",
    "    return index\n",
    "\n",
    "#detecting_outliers(tidy(mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c304cfcabfdae323170bf817c9278444",
     "grade": true,
     "grade_id": "cell-231217b55b6ef843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(type(detecting_outliers(tidy(mn))), list)\n",
    "assert_equal(all(isinstance(i, int) for i in detecting_outliers(tidy(mn))), True)\n",
    "assert_equal(len(detecting_outliers(tidy(mn))) > 0 and len(detecting_outliers(tidy(mn))) < .05*tidy(mn).shape[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e5ca4b902e63d96e5376adc95c8302d",
     "grade": false,
     "grade_id": "cell-8c4530b128b5c186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. Analytical part\n",
    "Discuss the implications. \n",
    "\n",
    "- What is the chosen outlier-detection technique? Explain it using your own words in 3-4 sentences.\n",
    "- Describe the outliers detected: How many? How do they relate to the typical, non-outlier values in the remaining dataset?\n",
    "- What could be one reason these outliers appear in the dataset? How would you treat them further?\n",
    "\n",
    "Write your answer in the markdown cell below. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e93305db24e2fc3800f252b797c2ad7",
     "grade": true,
     "grade_id": "cell-254db63097c3ed40",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Q1 (25th percentile): 19.0, Q3 (75th percentile): 36.0\n",
    "IQR: 17.0, Min value: 10.5, Max value: 44.5\n",
    "\n",
    "##### The inter-quantile-range (IQR)\n",
    "I started with computing 25th percentile (Q1 = 19.0) and the 75th percentile (Q3 = 36.0) of the data, IQR (= 17) is calculated as the difference between Q3 and Q1. Any data points that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are considered outliers. This method is effective for highlighting data wich is out of range of central quantile.\n",
    "\n",
    "##### Outliers Detected\n",
    "From the output of the function, the range for non-outliers was calculated to be between 10.5 and 44.5. I used distance 0.5 and in the end I have a list consisting indexes of 44 outliners, which are either 0 or exceeded 60, with max value 82. These outliers are not typical for the dataset, as most values are concentrated between 20 and 40.\n",
    "##### Reasons for Outliers\n",
    "I would say, that otliners in my dataset are caused by:\n",
    "- Natural Variation: Sometimes, outliers represent real but rare events or exceptional cases (e.g., a highly efficient vehicle).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
